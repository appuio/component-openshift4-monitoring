= Parameters

The parent key for all of the following parameters is `openshift4_monitoring`.

== `manifests_version`

[horizontal]
type:: string
default:: `release-4.9`

Select which version of the upstream alerting (and recording) rules should be used by the component.
This parameter must be changed to match the cluster's OCP4 minor version.

[TIP]
====
We recommend setting this parameter based on the reported OpenShift version which can be found in the cluster's https://syn.tools/syn/SDDs/0027-dynamic-cluster-facts.html[dynamic facts].

[source,yaml]
----
manifests_version: release-${dynamic_facts:openshiftVersion:Major}.${dynamic_facts:openshiftVersion:Minor}
----
====

== `defaultConfig`

[horizontal]
type:: dictionary
default::
+
[source,yaml]
----
nodeSelector:
  node-role.kubernetes.io/infra: ''
----

A dictionary holding the default configuration which should be applied to all components.


== `upstreamRules.networkPlugin`

[horizontal]
type:: string
default:: `openshift-sdn`

Choose either `openshift-sdn` or `ovn-kubernetes` depending on the installed network plugin.
If a custom network plugin is used, set any other string as the value for this parameter.
This ensures neither openshift-sdn nor OVN-Kubernetes monitoring rules are deployed.


== `upstreamRules.elasticsearchOperator`

[horizontal]
type:: boolean
default:: `true`

WARNING: This parameter is deprecated and will be removed in a future version of the component.

Whether the Elasticsearch Operator Prometheus rules should be included or not.

Component `openshift4-logging` starting with version v1.7.0 manages the Elasticsearch Operator Prometheus rules itself.
This parameter doesn't have an effect if the inventory includes component `openshift4-logging` with version v1.7.0 or newer.
For such inventories, this component will never include the Elasticsearch Operator Prometheus rules.

== `upstreamRules.clusterSamplesOperator`

[horizontal]
type:: boolean
default:: `true`

Whether the Cluster Samples Operator prometheus rules should be included or not.


== `enableUserWorkload`

[horizontal]
type:: boolean
default:: `true`

A parameter to enable https://docs.openshift.com/container-platform/latest/monitoring/enabling-monitoring-for-user-defined-projects.html[monitoring for user-defined projects].

== `configs`

[horizontal]
type:: dictionary
default::
+
[source,yaml]
----
prometheusK8s:
  remoteWrite: []
  _remoteWrite: {}
  externalLabels:
    cluster_id: ${cluster:name}
    tenant_id: ${cluster:tenant}
  retention: 8d
  volumeClaimTemplate:
    spec:
      resources:
        requests:
          storage: 50Gi
alertmanagerMain:
  volumeClaimTemplate:
    spec:
      resources:
        requests:
          storage: 2Gi
----

A dictionary holding the configurations for the https://docs.openshift.com/container-platform/latest/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack_configuring-the-monitoring-stack[monitoring components].

See the https://docs.openshift.com/container-platform/latest/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html[OpenShift docs] for available parameters.

This table shows the monitoring components you can configure and the keys used to specify the components:

[options="header"]
|====
|Component|Key
|Prometheus Operator|`prometheusOperator`
|Prometheus|`prometheusK8s`
|Alertmanager|`alertmanagerMain`
|kube-state-metrics|`kubeStateMetrics`
|openshift-state-metrics|`openshiftStateMetrics`
|Grafana|`grafana`
|Telemeter Client|`telemeterClient`
|Prometheus Adapter|`k8sPrometheusAdapter`
|Thanos Querier|`thanosQuerier`
|====

=== `configs.prometheusK8s._remoteWrite`

[horizontal]
type:: dictionary
default:: `{}`
example::
+
[source,yaml]
----
_remoteWrite:
  example:
    url: https://prometheus.example.com/api/v1/write
    headers:
      "X-Scope-OrgID": example
    writeRelabelConfigs:
      - sourceLabels: ['syn']
        regex: '.+'
        action: keep
    basicAuth:
      username:
        name: remote-write
        key: username
      password:
        name: remote-write
        key: password
----

A dictionary holding the remote write configurations for the Prometheus component.
The key is the name of the configuration, the value is the content of the configuration.

The remote write configuration will be appended to the `configs.prometheusK8s.remoteWrite` parameter for backwards compatibility.


== `configsUserWorkload`

[horizontal]
type:: dictionary
default::
+
[source,yaml]
----
alertmanager:
  enabled: true
  enableAlertmanagerConfig: true
  volumeClaimTemplate: ${openshift4_monitoring:configs:alertmanagerMain:volumeClaimTemplate}
prometheusOperator: {}
prometheus:
  retention: 8d
  volumeClaimTemplate: ${openshift4_monitoring:configs:prometheusK8s:volumeClaimTemplate}
thanosRuler: {}
----

A dictionary holding the configurations for the https://docs.openshift.com/container-platform/latest/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack_configuring-the-monitoring-stack[user workload monitoring components].

By default, we configure the user workload monitoring Prometheus and Alertmanager to inherit the `volumeClaimTemplate` specifications from the cluster-monitoring config.
This allows users to configure the default storageclass and volume size of both monitoring stacks through the cluster-monitoring config.

This table shows the monitoring components you can configure and the keys used to specify the components:

[options="header"]
|====
|Component|Key|Note
|Alertmanager|`alertmanager`|Only on OpenShift 4.11 and newer
|Prometheus Operator|`prometheusOperator`|
|Prometheus|`prometheus`|
|Thanos Ruler|`thanosRuler`|
|====

== `alertManagerConfig`

[horizontal]
type:: dictionary
default::
+
[source,yaml]
----
route:
  group_wait: 0s
  group_interval: 5s
  repeat_interval: 10m
inhibit_rules:
  # Don't send warning or info if a critical is already firing
  - target_match_re:
      severity: warning|info
    source_match:
      severity: critical
    equal:
      - namespace
      - alertname
  # Don't send info if a warning is already firing
  - target_match_re:
      severity: info
    source_match:
      severity: warning
    equal:
      - namespace
      - alertname
----

A dictionary holding the configuration for the AlertManager.

See the https://docs.openshift.com/container-platform/latest/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html#configuring-alertmanager[OpenShift docs] for available parameters.

== `alerts`

[horizontal]
type:: dictionary

Configuration parameters related to influence the resulting alert rules.

=== `ignoreNames`

[horizontal]
type:: list
default:: `[]`

List of https://github.com/prometheus-operator/kube-prometheus[kube-prometheus] alert rule names to be dropped.

=== `customAnnotations`

[horizontal]
type:: dict
default:: `{}`

Maps alert names to sets of custom annotations.
Allows configuring custom annotations for individual alerts.

Example:

[source,yaml]
----
customAnnotations:
  Watchdog:
    runbook_url: https://www.google.com/?q=Watchdog
----

=== `patchRules`
type:: dict
keys:: potential values of parameter `manifests_versions`
default:: See https://github.com/appuio/component-openshift4-monitoring/blob/master/class/defaults.yml[`class/defaults.yml` on GitHub]

The parameter `patchRules` allows users to customize upstream alerts.
The component expects that top-level keys in the parameter correspond to values of parameter `manifests_versions`.
This enables users to selectively patch upstream alerts for a particular OpenShift 4 version.

For each version, the component expects alert names as keys and any alert configuration as values.
See the Prometheus https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting rules documentation] for extended documentation on configuring alerting rules.

Example:

[source,yaml]
----
patchRules:
  release-4.9:
    SystemMemoryExceedsReservation:
      for: 30m
----

=== `ignoreUserWorkload`

[horizontal]
type:: list
default:: `[]`

A list of alerting rules for which the component should patch the `expr` and `annotations.description` fields to ensure they don't alert for the user workload monitoring stack.

By default, we don't turn off any alerts for the user workload monitoring stack.

The parameter supports removing entries by providing the entry to remove prefixed with `~`.
The parameter can be completely cleared with the following config:

[source,yaml]
----
parameters:
  openshift4_monitoring:
    alerts:
      ~ignoreUserWorkload: []
----


== `silence`

[horizontal]
type:: dict

Parameters to configure the silence CronJob.


== `silence.silences`

[horizontal]
type:: dict
default::
+
[source,yaml]
----
"Silence non syn alerts":
  matchers:
    - name: alertname
      value: ".+"
      isRegex: true
    - name: syn
      value: ""
      isRegex: false
----

Contains the list of silences to be applied.
The key is used as the comment of the silence and the value is a dictionary which is passed to Alertmanager.

Silences removed from the hierarchy stay active in Alertmanager for up to a year until they expire.

Silences all non-SYN alerts by default.

=== `schedule`

[horizontal]
type:: string
default:: '0 */4 * * *'

Schedule of the CronJob in cron syntax.

=== `serviceAccountName`

[horizontal]
type:: string
default:: prometheus-k8s

Name of the service account used when running the silence job.
The service account must have permission to access the Alertmanager service through its oAuth proxy.

=== `servingCertsCABundleName`

[horizontal]
type:: string
default:: serving-certs-ca-bundle

Name of the config map containing the CA bundle of the Alertmanager service.

=== `jobHistoryLimit`

[horizontal]
type:: dict

Parameters to configure the numbers of silence job objects to keep.

==== `failed`

[horizontal]
type:: number
default:: 3

Number of failed jobs to keep.

==== `successful`

[horizontal]
type:: number
default:: 3

Number of successful jobs to keep.

== `capacityAlerts`

[horizontal]
type:: dict

This parameter allows users to enable and configure alerts for capacity management.
The capacity alerts are enabled by default and can be disabled completely by setting the key `capacityAlerts.enabled` to `false`.
Predictive alerts are disabled by default and can be enabled individually as shown below by setting `ExpectClusterCpuUsageHigh.enabled` to `true`.

The dictionary will be transformed into a `PrometheusRule` object by the component.

The component provides 10 alerts that are grouped in four groups.
You can disable or modify each of these alert rules individually.
The fields in these rules will be added to the final `PrometheusRule`, with the exception of `expr`.
The `expr` field contains fields which can be used to tune the default alert rule.
Alternatively the default rule can be completely overwritten by setting the `expr.raw` field (see example below).
See xref:explanations/resource_management.adoc[Resource Management] for an explanation for every alert rule.

Example:

[source,yaml]
---
capacityAlerts:
  enabled: true <1>
  groups:
    PodCapacity:
      rules:
        TooManyPods:
          annotations:
            message: 'The number of pods is too damn high' <2>
          for: 3h <3>
        ExpectTooManyPods:
          expr: <4>
            range: '2d'
            predict: '5*24*60*60'

    ResourceRequests:
      rules:
        TooMuchMemoryRequested:
          enabled: true
          expr:
            raw: sum(kube_pod_resource_request{resource="memory"}) > 9000*1024*1024*1024 <5>
    CpuCapacity:
      rules:
        ClusterCpuUsageHigh:
          enabled: false <6>
        ExpectClusterCpuUsageHigh:
          enabled: false <6>
    UnusedCapacity:
      rules:
        ClusterHasUnusedNodes:
          enabled: false <7>
---
<1> Enables capacity alerts
<2> Changes the alert message for the pod capacity alert
<3> Only alerts for pod capacity if it fires for 3 hours
<4> Change the pod count prediction to look at the last two days and predict the value in five days
<5> Completely overrides the default alert rule and alerts if the total memory request is over 9000 GB
<6> Disables both CPU capacity alert rules
<7> Disables alert if the cluster has unused nodes.


== `rules`

[horizontal]
type:: dict
default:: `{}`

This parameter allows users to configure additional Prometheus rules to deploy on the cluster.

Each key-value pair in the dictionary is transformed into a `PrometheusRule` object by the component.

The component expects that values are dicts themselves and expects that keys in those dicts are prefixed with `record:` or `alert:` to indicate whether the rule is a recording or alerting rule.
The component will transform the keys into fields in the resulting rule by taking the prefix as the field name and the rest of the key as the field value.
For example, key `"record:sum:some:metric:5m"` would be transformed into `record: sum:some:metric:5m` which should define a recording rule with name `sum:some:metric:5m`.
This field is then merged into the provided value which should be a valid rule definition.

See the Prometheus docs for supported configurations for https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/[recording] and https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[alerting] rules.


Example:

[source,yaml]
---
rules:
  generic-rules:
    "alert:ContainerOOMKilled":
      annotations:
        message: A container ({{$labels.container}}) in pod {{ $labels.namespace }}/{{ $labels.pod }} was OOM killed
      expr: |
        kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
      labels:
        source: https://git.vshn.net/swisscompks/syn-tenant-repo/-/blob/master/common.yml
        severity: devnull
---

== Example

[source,yaml]
----
defaultConfig:
  nodeSelector:
    node-role.kubernetes.io/infra: ''
configs:
  prometheusK8s:
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 100Gi
alerts:
  ignoreNames:
    - KubeAPIErrorsHigh
    - KubeClientErrors
----

== `syn_monitoring`

This parameter allows users to enable the component's monitoring configuration.
Currently the component has support for deploying custom `ServiceMonitors` on clusters which use component `prometheus` to manage a custom monitoring stack.

=== `enabled`

[horizontal]
type:: boolean
default:: `true`

Whether to deploy monitoring configurations.
If this parameter is set to `true`, the component will check whether component `prometheus` is present on the cluster.
If the component is missing, no configurations will be deployed regardless of the value of this parameter.

=== `instance`

[horizontal]
type:: string
default:: `null`

This parameter can be used to indicate which custom Prometheus instance should pick up the configurations managed by the component.

If the parameter is set to the empty string, the default instance configured for component `prometheus` will be used.


== `secrets`

[horizontal]
type:: dict
default:: `{}`

A dict of secrets to create in the namespace.
The key is the name of the secret, the value is the content of the secret.
The value must be a dict with a key `stringData` which is a dict of key/value pairs to add to the secret.

== `cortex_tenant_ns_label`

[horizontal]
type:: dict

Parameters to configure the cortex-tenant-ns-label reverse proxy.

=== `enabled`

[horizontal]
type:: boolean
default:: `false`

Enable or disable the cortex-tenant-ns-label reverse proxy.

=== `requests`

[horizontal]
type:: dict
default::
+
[source,yaml]
----
cpu: '500m'
memory: '256Mi'
----

Configure the reserved resources for the cortex-tenant-ns-label reverse proxy.

=== `limits`

[horizontal]
type:: dict
default::
+
[source,yaml]
----
cpu: '5'
memory: '256Mi'
----

Configure the maximum resources for the cortex-tenant-ns-label reverse proxy.

=== `config`

[horizontal]
type:: string
default:: ``
example::
+
[source,yaml]
----
listen: 0.0.0.0:8080
target: https://metrics-receive.appuio.net/api/v1/push
log_level: warn
auth:
  egress:
    username: ?{vaultkv:__shared__/__shared__/metrics-receive-appuio-net-remote-write/username}
    password: ?{vaultkv:__shared__/__shared__/metrics-receive-appuio-net-remote-write/password}
  namespace:
    tenant_label: 'appuio.io/organization'
----

Configure the cortex-tenant-ns-label reverse proxy. See https://github.com/vshn/cortex-tenant-ns-label for more details.

